##-- Cluster definition
ClusterName=reindeerpizza
#-- Compute nodes
NodeName=cn[1-10] Sockets=8 CoresPerSocket=2 ThreadsPerCore=2 RealMemory=112000 Gres=gpu:2
NodeName=cn[11-12] Sockets=8 CoresPerSocket=2 ThreadsPerCore=2 RealMemory=112000 Gres=gpu:2

#-- Partitions
PartitionName=main Nodes=cn[1-10] Default=YES MaxTime=INFINITE State=UP OverSubscribe=NO TRESBillingWeights="CPU=2.6,Mem=0.25G,GRES/gpu=24.0"
PartitionName=main-beeond Nodes=cn[1-10] Default=NO MaxTime=INFINITE State=UP OverSubscribe=EXCLUSIVE TRESBillingWeights="CPU=2.6,Mem=0.25G,GRES/gpu=24.0"
PartitionName=private Nodes=cn[11-12] Default=NO MaxTime=INFINITE State=UP OverSubscribe=NO Hidden=YES AllowQos=admin

SlurmUser=slurm                                # SlurmUser sounds generic but in fact it's defined as slurmctl's user
LaunchParameters=enable_nss_slurm

#-- Slurmctl
SlurmctldHost=slurmctl(10.10.2.152)
SlurmctldDebug=debug
SlurmctldParameters=enable_configless
DebugFlags=Script,Gang,SelectType
StateSaveLocation=/var/spool/slurmctl
SlurmctldPidFile=/var/run/slurmctl/d.pid
SlurmctldLogFile=/var/log/slurm/slurmctld.log
PrologSlurmctld=/etc/slurm/prolog-slurmctld
EpilogSlurmctld=/etc/slurm/epilog-slurmctld
UnkillableStepTimeout=300

#-- Slurmd
#SlurmdPidFile=/var/run/slurmd.pid
#SlurmdSpoolDir=/var/spool/slurm/d
SlurmdDebug=verbose
SlurmdLogFile=/var/log/slurm/slurmd.log
SrunPortRange=60001-63000
#Prolog=/etc/slurm/prologs/mps.pl

#-- Default ressources allocation
DefCpuPerGPU=4
DefMemPerCpu=7000
SchedulerTimeSlice=60

###Â Cluster settings
# MPI stacks running over Infiniband or OmniPath require the ability to allocate more
# locked memory than the default limit. Unfortunately, user processes on login nodes
# may have a small memory limit (check it by ulimit -a) which by default are propagated
# into Slurm jobs and hence cause fabric errors for MPI.
PropagateResourceLimitsExcept=MEMLOCK

ProctrackType=proctrack/cgroup
#TaskPlugin=task/affinity,task/cgroup
TaskPlugin=task/cgroup
SwitchType=switch/none
MpiDefault=pmix_v2
ReturnToService=2 #temp
GresTypes=gpu
PreemptType=preempt/qos
#PreemptMode=SUSPEND,GANG
PreemptMode=REQUEUE
#JobRequeue=1
PreemptExemptTime=-1
#TaskProlog=/etc/slurm/taskprolog
MailProg=/usr/bin/fakemail
Prolog=/etc/slurm/prolog.d/*
Epilog=/etc/slurm/epilog.d/*

#-- Scheduling
#SchedulerType=sched/builtin
SchedulerType=sched/backfill
#SchedulerParameters=sched_interval=10
SelectType=select/cons_tres
#SelectType=select/linear
#SelectTypeParameters=CR_Memory
#SelectTypeParameters=CR_Core
#SelectTypeParameters=CR_CPU
#SelectTypeParameters=CR_Core_Memory
SelectTypeParameters=CR_CPU_Memory

#-- Multifactor priority
PriorityType=priority/multifactor
# The larger the job, the greater its job size priority.
PriorityFavorSmall=NO
# The job's age factor reaches 1.0 after waiting in the
# queue for 2 weeks.
#PriorityMaxAge=14-0
# This next group determines the weighting of each of the
# components of the Multi-factor Job Priority Plugin.
# The default value for each of the following is 1.
PriorityWeightAge=0
PriorityWeightFairshare=0
PriorityWeightJobSize=0
PriorityWeightPartition=0
PriorityWeightQOS=100


#-- Accounting
AccountingStorageType=accounting_storage/slurmdbd
AccountingStorageHost=slurmdbd.csquare.gcloud
AccountingStoragePort=6819
AccountingStorageTRES=gres/gpu,gres/gpu:geforce_gtx_1080_ti
AccountingStoreFlags=job_comment,job_env,job_script
AccountingStorageEnforce=associations,limits,qos
PriorityDecayHalfLife=0
PriorityUsageResetPeriod=MONTHLY

JobCompType=jobcomp/report
JobCompLoc=https://slurm:f_4UyDHC7%21w%24Cc2KSHFv@slurm.accounting.csquare.gcloud/report
#JobCompType=jobcomp/elasticsearch
#JobCompLoc=http://10.172.0.30:9200/slurm/_doc
#DebugFlags=Elasticsearch
#JobAcctGatherType=jobacct_gather/linux
JobAcctGatherType=jobacct_gather/cgroup

#-- Multi Authentication
AuthType=auth/munge
AuthAltTypes=auth/jwt

# Federation
FederationParameters=fed_display
